{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TARS Colab: Real-Usecase Agent Self-Improvement Evaluation\n",
        "\n",
        "This notebook tests whether the **same support agent** improves over an ordered sequence of real-world-style customer support conversations.\n",
        "\n",
        "Use case: an ecommerce support assistant evolving from v1.0 to v1.5.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup\n",
        "\n",
        "If you are running in Colab, clone your repo first. If you're already inside the repo, skip the clone cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional (Colab): clone the repo\n",
        "# !git clone https://github.com/<your-org-or-user>/tars.git\n",
        "# %cd tars\n",
        "\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -e .\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Inspect the real-usecase dataset\n",
        "\n",
        "Dataset file: `examples/customer_support_progression.jsonl`.\n",
        "Each record is a full conversation at a point in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "dataset_path = Path('examples/customer_support_progression.jsonl')\n",
        "rows = [json.loads(line) for line in dataset_path.read_text().splitlines() if line.strip()]\n",
        "\n",
        "print(f'Loaded {len(rows)} conversations')\n",
        "print('Conversation IDs in order:')\n",
        "for r in rows:\n",
        "    print('-', r['timestamp'], r['conversation_id'], r.get('metadata', {}).get('agent_version'))\n",
        "\n",
        "print('\\nSample first conversation:')\n",
        "print(json.dumps(rows[0], indent=2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Offline deterministic test (no Gemini key required)\n",
        "\n",
        "This validates repository functionality end-to-end by monkeypatching `GeminiEvaluator` with a deterministic progression scorer.\n",
        "The scorer is heuristic-based and should output an improving trajectory for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tars_analyzer import analyzer\n",
        "from tars_analyzer.models import ConversationProgress, ProgressionEvaluation\n",
        "\n",
        "class HeuristicProgressionEvaluator:\n",
        "    def __init__(self, model='gemini-2.0-flash'):\n",
        "        self.model = model\n",
        "\n",
        "    def evaluate_progression(self, conversations):\n",
        "        conversations = sorted(conversations, key=lambda c: c.timestamp)\n",
        "\n",
        "        def score_text(text: str) -> float:\n",
        "            text_l = text.lower()\n",
        "            score = 4.5\n",
        "            if any(k in text_l for k in ['sorry', 'thanks', 'appreciate']):\n",
        "                score += 0.8\n",
        "            if any(k in text_l for k in ['i checked', 'i found', 'reviewed', 'logs']):\n",
        "                score += 1.0\n",
        "            if any(k in text_l for k in ['done', 'submitted', 'confirmed', 'generated', 'scheduled']):\n",
        "                score += 1.2\n",
        "            if any(k in text_l for k in ['timeline', 'eta', 'business days', 'reference', '#']):\n",
        "                score += 0.9\n",
        "            if any(k in text_l for k in ['contingency', 'fallback', 'options', 'trade-off']):\n",
        "                score += 0.6\n",
        "            return min(10.0, score)\n",
        "\n",
        "        qualities = []\n",
        "        for convo in conversations:\n",
        "            agent_text = ' '.join(t.content for t in convo.turns if t.role.lower() == 'agent')\n",
        "            qualities.append(round(score_text(agent_text), 2))\n",
        "\n",
        "        ranks = {idx: rank for rank, idx in enumerate(sorted(range(len(qualities)), key=lambda i: qualities[i]), start=1)}\n",
        "\n",
        "        per = []\n",
        "        for i, convo in enumerate(conversations):\n",
        "            prev = qualities[i - 1] if i > 0 else qualities[i]\n",
        "            per.append(\n",
        "                ConversationProgress(\n",
        "                    conversation_id=convo.conversation_id,\n",
        "                    rank=ranks[i],\n",
        "                    overall_agent_quality=qualities[i],\n",
        "                    improvement_vs_previous=round(0.0 if i == 0 else qualities[i] - prev, 2),\n",
        "                    notes='Heuristic offline score based on empathy, actionability, and follow-through.',\n",
        "                )\n",
        "            )\n",
        "\n",
        "        delta = qualities[-1] - qualities[0]\n",
        "        label = 'improving' if delta > 0.4 else 'declining' if delta < -0.4 else 'flat'\n",
        "        return ProgressionEvaluation(\n",
        "            overall_summary='Offline heuristic progression evaluation completed.',\n",
        "            trajectory_label=label,\n",
        "            trajectory_confidence=7.0,\n",
        "            per_conversation=per,\n",
        "        )\n",
        "\n",
        "original = analyzer.GeminiEvaluator\n",
        "analyzer.GeminiEvaluator = HeuristicProgressionEvaluator\n",
        "try:\n",
        "    offline_report = analyzer.analyze_conversations(dataset_path, 'output_offline')\n",
        "finally:\n",
        "    analyzer.GeminiEvaluator = original\n",
        "\n",
        "print('Offline trajectory:', offline_report['trajectory']['label'])\n",
        "print('Offline first\u2192last delta:', offline_report['trend_delta_first_to_last'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "print(json.dumps(offline_report['trajectory'], indent=2))\n",
        "!cat output_offline/report.md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Optional live Gemini run (recommended)\n",
        "\n",
        "This is the actual model-based evaluation for your real-usecase dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if not os.getenv('GEMINI_API_KEY'):\n",
        "    os.environ['GEMINI_API_KEY'] = getpass('Enter GEMINI_API_KEY: ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m tars_analyzer.cli examples/customer_support_progression.jsonl --out output_gemini --model gemini-2.0-flash\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "gemini_report = json.loads(Path('output_gemini/report.json').read_text())\n",
        "print('Gemini trajectory:', gemini_report['trajectory']['label'])\n",
        "print('Gemini first\u2192last delta:', gemini_report['trend_delta_first_to_last'])\n",
        "print('Gemini quality scores:', gemini_report['overall_agent_quality_scores'])\n",
        "\n",
        "!cat output_gemini/report.md\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Visualize progression\n",
        "\n",
        "This chart should make the self-improvement trend easy to inspect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scores = gemini_report['overall_agent_quality_scores']\n",
        "labels = [a['conversation_id'] for a in gemini_report['analyses']]\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(labels, scores, marker='o')\n",
        "plt.xticks(rotation=30, ha='right')\n",
        "plt.ylabel('Overall Agent Quality (0-10)')\n",
        "plt.xlabel('Conversation ID (time ordered)')\n",
        "plt.title(f\"Trajectory: {gemini_report['trajectory']['label']} | \u0394 first\u2192last = {gemini_report['trend_delta_first_to_last']:+}\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}